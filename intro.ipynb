{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2109d367",
   "metadata": {},
   "source": [
    "## Big Data Analytics with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5108276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.appName('my_spark').getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4061e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV\n",
    "census_adult = spark.read.csv(\"adult_reduced.csv\")\n",
    "\n",
    "# Show the DataFrame\n",
    "census_adult.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average salary for entry level in Canada\n",
    "CA_jobs = ca_salaries_df.filter(ca_salaries_df['company_location'] == \"CA\").filter(ca_salaries_df[\n",
    "    'experience_level']== \"EN\").groupBy().avg(\"salary_in_usd\")\n",
    "\n",
    "# Show the result\n",
    "CA_jobs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframe\n",
    "census_df = spark.read.json(\"adults.json\")\n",
    "\n",
    "# Filter rows based on age condition\n",
    "salary_filtered_census = census_df.filter(census_df['age']>40)\n",
    "\n",
    "# Show the result\n",
    "salary_filtered_census.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5468904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# Fill in the schema with the columns you need from the exercise instructions\n",
    "schema = StructType([StructField(\"age\",IntegerType()),\n",
    "                     StructField(\"education_num\",IntegerType()),\n",
    "                     StructField(\"marital_status\",StringType()),\n",
    "                     StructField(\"occupation\",StringType()),\n",
    "                     StructField(\"income\",StringType()),\n",
    "                    ])\n",
    "\n",
    "# Read in the CSV, using the schema you defined above\n",
    "census_adult = spark.read.csv(\"adult_reduced_100.csv\", sep=',', header=False, schema=schema)\n",
    "\n",
    "# Print out the schema\n",
    "census_adult.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944a46b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the data\n",
    "airports.show()\n",
    "\n",
    "# .withColumnRenamed() renames the \"faa\" column to \"dest\"\n",
    "airports = airports.withColumnRenamed(\"faa\", \"dest\")\n",
    "\n",
    "# Join the DataFrames\n",
    "flights_with_airports = flights.join(airports, on='dest', how='leftouter')\n",
    "\n",
    "# Examine the new DataFrame\n",
    "flights_with_airports.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673b8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the function age_category as a UDF\n",
    "age_category_udf = udf(age_category, StringType())\n",
    "\n",
    "# Apply your udf to the DataFrame\n",
    "age_category_df_2 = age_category_df.withColumn(\"category\", age_category_udf (age_category_df[\"age\"]))\n",
    "\n",
    "# Show df\n",
    "age_category_df_2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a40dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = spark.read.csv(\"salaries.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Convert DataFrame to RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Show the RDD's contents\n",
    "rdd.collect()\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7332750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from the df_salaries\n",
    "rdd_salaries = df_salaries.rdd\n",
    "\n",
    "# Collect and print the results\n",
    "print(rdd_salaries.collect())\n",
    "\n",
    "# Group by the experience level and calculate the maximum salary\n",
    "dataframe_results = df_salaries.groupby(\"experience_level\").agg({\"salary_in_usd\": 'max'})\n",
    "\n",
    "# Show the results\n",
    "dataframe_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf05e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register as a view\n",
    "df.createOrReplaceTempView(\"data_view\")\n",
    "\n",
    "# Advanced SQL query: Calculate total salary by Position\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT Position, SUM(Salary) AS Total_Salary\n",
    "    FROM data_view\n",
    "    GROUP BY Position\n",
    "    ORDER BY Total_Salary DESC\n",
    "    \"\"\"\n",
    ")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9621f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table \"people\"\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# Select the names from the temporary table people\n",
    "query = \"\"\"SELECT name FROM people\"\"\"\n",
    "\n",
    "# Assign the result of Spark's query to people_df_names\n",
    "people_df_names = spark.sql(query)\n",
    "\n",
    "# Print the top 10 names of the people\n",
    "people_df_names.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdcc2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view of salaries_table\n",
    "salaries_df.createOrReplaceTempView('salaries_table')\n",
    "\n",
    "# Construct the \"query\"\n",
    "query = '''SELECT job_title, salary_in_usd FROM salaries_table WHERE company_location == \"CA\"'''\n",
    "\n",
    "# Apply the SQL \"query\"\n",
    "canada_titles = spark.sql(query)\n",
    "\n",
    "# Generate basic statistics\n",
    "canada_titles.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d766109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum salaries for small companies\n",
    "salaries_df.filter(salaries_df.company_size == \"S\").groupBy().min(\"salary_in_usd\").show()\n",
    "\n",
    "# Find the maximum salaries for large companies\n",
    "salaries_df.filter(salaries_df.company_size == \"L\").groupBy().max(\"salary_in_usd\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b6f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average salaries at large us companies\n",
    "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\").groupBy().avg('salary_in_usd')\n",
    "\n",
    "#set a large companies variable for other analytics\n",
    "large_companies=salaries_df.filter(salaries_df.company_size == \"L\").filter(salaries_df.company_location == \"US\")\n",
    "\n",
    "# Total salaries in usd\n",
    "large_companies.groupBy().sum('salary_in_usd').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68880f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SparkSession from pyspark.sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create my_spark\n",
    "my_spark = SparkSession.builder.appName(\"final_spark\").getOrCreate()\n",
    "\n",
    "# Print my_spark\n",
    "print(my_spark)\n",
    "\n",
    "# Load dataset into a DataFrame\n",
    "df = my_spark.createDataFrame(data, schema=columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the DataFrame\n",
    "df.cache()\n",
    "\n",
    "# Perform aggregation\n",
    "agg_result = df.groupBy(\"Department\").sum(\"Salary\")\n",
    "agg_result.show()\n",
    "\n",
    "# Analyze the execution plan\n",
    "agg_result.explain()\n",
    "\n",
    "# Uncache the DataFrame\n",
    "df.unpersist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
